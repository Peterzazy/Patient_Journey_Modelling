{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from FINAL.Configs.config import *\n",
    "from FINAL.Models.PJ.models import Model, Model_LAHST\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from FINAL.Data.Multi_Modal_Dataset import Multi_Modal_Dataset, tolerant_collate\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "from Multi_Modal_Continuous.utils import get_tokenizer\n",
    "from Multi_Modal_Continuous.metrics import MyMetrics\n",
    "from FINAL.loop_pj import inference\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from FINAL.loop_pj import select_sequence\n",
    "import torch.cuda.amp as amp\n",
    "import time as timing\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch"
   ],
   "id": "4346c6331c6168e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6073f7b7091e7781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_gate_importance(gates, type_gates, note_idx=None, top_k=10, figsize=(12, 4), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot gating values for events enriching a particular note.\n",
    "\n",
    "    Parameters:\n",
    "    - gates: List or 1D np.array of gating values.\n",
    "    - note_idx: Optional, index of the note (to annotate title).\n",
    "    - top_k: Number of top gating values to annotate.\n",
    "    - figsize: Size of the plot.\n",
    "    - save_path: Path to save figure if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    gates = np.array(gates.detach().to(\"cpu\"))\n",
    "    event_ids = np.arange(len(gates))\n",
    "\n",
    "    # Define colors\n",
    "    color_map = {'Lab': 'green', 'Drug': 'orange', 'Microbio': 'red'}\n",
    "    default_color = 'skyblue'\n",
    "    fallback_color = 'gray'\n",
    "    bar_colors = [default_color] * len(gates)\n",
    "\n",
    "    # Identify top-k events and assign their color\n",
    "    top_indices = np.argsort(gates)[-top_k:][::-1]\n",
    "    used_types = set()\n",
    "    for i in top_indices:\n",
    "        event_type = type_gates[i]\n",
    "        used_types.add(event_type)\n",
    "        bar_colors[i] = color_map.get(event_type, fallback_color)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.bar(event_ids, gates, color=bar_colors, edgecolor='black')\n",
    "\n",
    "    # Annotate top-k indices\n",
    "    for i in top_indices:\n",
    "        plt.text(i, gates[i] + 0.01, f'{i}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "    # Add legend for top-k event types\n",
    "    legend_handles = [Patch(color=color_map[etype], label=etype) for etype in sorted(used_types)]\n",
    "    plt.legend(handles=legend_handles, title=\"Event Type (Top-10)\", loc='upper right')\n",
    "\n",
    "    plt.xlabel(\"Event Index\")\n",
    "    plt.ylabel(\"Gating Value\")\n",
    "    title = f\"Gating Values for Note {note_idx}\" if note_idx is not None else \"Gating Values per Event\"\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_vector_at_index(list_of_lists, event_types, index):\n",
    "    current_start = 0\n",
    "    j = 0\n",
    "    for sublist in list_of_lists:\n",
    "        num_rows = len(sublist)\n",
    "        temp_event_types = [event_types[j][i] for i in range(len(event_types[j])) if event_types[j][i]!='Text']\n",
    "        if index < current_start + num_rows:\n",
    "            return sublist[index - current_start], temp_event_types\n",
    "        current_start += num_rows\n",
    "        j+=1\n",
    "    raise IndexError(\"Index out of range\")\n",
    "\n",
    "def set_size(width, fraction=0.8):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Document textwidth or columnwidth in pts\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width * fraction\n",
    "\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5**.5 - 1) / 2\n",
    "\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio\n",
    "\n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    "\n",
    "    return fig_dim\n",
    "def return_attn_scores(lwan, encoding, all_tokens=True, cutoffs=None):\n",
    "    # encoding: Tensor of size (Nc x T) x H\n",
    "    # mask: Tensor of size Nn x (Nc x T) x H\n",
    "    # temporal_encoding = Nn x (N x T) x hidden_size\n",
    "    T = lwan.seq_len\n",
    "    if not lwan.all_tokens:\n",
    "        T = 1  # only use the [CLS]-token representation\n",
    "    Nc = int(encoding.shape[0] / T)\n",
    "    H = lwan.hidden_size\n",
    "    Nl = lwan.num_labels\n",
    "\n",
    "    # label query: shape L, H\n",
    "    # encoding: hape NcxT, H\n",
    "    # query shape:  Nn, L, H\n",
    "    # key shape: Nn, Nc*T, H\n",
    "    # values shape: Nn, Nc*T, H\n",
    "    # key padding mask: Nn, Nc*T (true if ignore)\n",
    "    # output: N, L, H\n",
    "    mask = torch.ones(size=(Nc, Nc * T), dtype=torch.bool).to(device=lwan.device)\n",
    "    for i in range(Nc):\n",
    "        mask[i, : (i + 1) * T] = False\n",
    "\n",
    "    # only mask out at 2d, 5d, 13d and no DS to reduce computation\n",
    "    # get list of cutoff indices from cutoffs dictionary\n",
    "\n",
    "    attn_output, attn_output_weights = lwan.multiheadattn.forward(\n",
    "        query=lwan.label_queries.repeat(mask.shape[0], 1, 1),\n",
    "        key=encoding.repeat(mask.shape[0], 1, 1),\n",
    "        value=encoding.repeat(mask.shape[0], 1, 1),\n",
    "        key_padding_mask=mask,\n",
    "        need_weights=True,\n",
    "    )\n",
    "\n",
    "    score = torch.sum(\n",
    "        attn_output\n",
    "        * lwan.label_weights.unsqueeze(0).view(\n",
    "            1, lwan.num_labels, lwan.hidden_size\n",
    "        ),\n",
    "        dim=2,\n",
    "    )\n",
    "    return attn_output_weights, score\n",
    "\n",
    "\n",
    "\n",
    "def load_token_dicts_from_json(filepath=\"token_dicts.json\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        loaded = json.load(f)\n",
    "\n",
    "    token_dicts = {}\n",
    "    for key, d in loaded.items():\n",
    "        if \"ind2tok\" in key:\n",
    "            # Convert keys back to integers\n",
    "            d = {int(k): v for k, v in d.items()}\n",
    "        token_dicts[key] = d\n",
    "\n",
    "    return token_dicts\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a JSON file.\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file {config_path} not found.\")\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    return config_data\n",
    "\n",
    "def view_model(model, output_param =True, output_struct = False):\n",
    "    if output_param:\n",
    "        total = 0\n",
    "        print(\"Detailed Parameter Breakdown:\")\n",
    "        print(\"--------------------------------\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param_count = param.numel()\n",
    "                print(f\"{name:60}: {param_count}\")\n",
    "                total += param_count\n",
    "        print(\"--------------------------------\")\n",
    "        print(f\"Total Trainable Parameters: {total:,}\")\n",
    "        return total\n",
    "    if output_struct:\n",
    "        print(\"Model Structure (Modules):\")\n",
    "        print(\"--------------------------------\")\n",
    "        for name, module in model.named_modules():\n",
    "            if name == \"\":\n",
    "                print(f\"{type(module).__name__}: [root module]\")\n",
    "            else:\n",
    "                print(f\"{type(module).__name__}: {name}\")\n"
   ],
   "id": "ed5c6d09d4b3eb0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config_path = \"/FINAL/Models/PJ/Saved_models/LAHST/baseline/config_MMULA_evaluate.json\"\n",
    "\n",
    "compute_all_set = True\n",
    "config = load_config(config_path)\n",
    "# qualitative_evaluation = config[\"qualitative_evaluation\"]\n",
    "\n",
    "print(\"loading tokens...\")\n",
    "token_dicts = load_token_dicts_from_json(config[\"event_tokens_file\"])\n",
    "tokenizer = get_tokenizer(config[\"base_checkpoint\"])\n",
    "\n",
    "val_set = Multi_Modal_Dataset(name=\"VALIDATION\", file_path=config[\"file_path\"], splits=config[\"splits\"],\n",
    "                               token_dicts=token_dicts,\n",
    "                               mimic_dir=config[\"mimic_dir\"], tokenizer=tokenizer,\n",
    "                               saved_path=config[\"saved_path\"])\n",
    "\n",
    "validation_loader = DataLoader(val_set, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                               collate_fn=tolerant_collate)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "# model = Model(config, device=device)\n",
    "model = Model_LAHST(config=config, device=device)\n",
    "\n",
    "param = view_model(model)\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    os.path.join(config[\"project_path\"], f\"Saved_models/LAHST/baseline/BEST_{config['run_name']}.pth\"),\n",
    "    map_location=torch.device(\"mps\"),\n",
    "    weights_only=False\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "976ebd7f438f15d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "do_sample = True\n",
    "\n",
    "if do_sample:\n",
    "    # evaluation metrics\n",
    "    my_metrics = MyMetrics(debug=config[\"debug\"])\n",
    "\n",
    "    # sample = val_set[605]  # hadm == 137969\n",
    "    sample = val_set[173]  # hadm == 111403\n",
    "    hadm, event_type_sequence, sequences, sequences_tokenized, timestamps, labels= sample\n",
    "\n",
    "    event_type_seq, seq, seq_tokenized, tstamp, input_ids, attention_mask, seq_ids, category_ids, cutoffs, pos_encodings, rev_encodings \\\n",
    "        = select_sequence(event_type_sequence, sequences, sequences_tokenized, timestamps, tokenizer,\n",
    "                          mode=\"eval\")  # to change to train\n",
    "\n",
    "    print(input_ids.shape)\n",
    "\n",
    "    labels = labels[: model.num_labels]\n",
    "    avail_docs = seq_ids.max().item() + 1\n",
    "    # note_end_chunk_ids = data[\"note_end_chunk_ids\"]\n",
    "    print(cutoffs)\n",
    "\n",
    "    complete_sequence_output = []\n",
    "    complete_gate_lists = []\n",
    "    # run through data in chunks of max_chunks\n",
    "    t = 0\n",
    "    for i in tqdm(range(0, input_ids.shape[0], model.max_chunks)):\n",
    "        # only get the document embeddings\n",
    "        sequence_output, gate_lists = model(\n",
    "            event_types=event_type_seq[t], seq_tokenized=seq_tokenized[t],\n",
    "            input_ids=input_ids[i: i + model.max_chunks].to(\n",
    "                device, dtype=torch.long\n",
    "            ),\n",
    "            attention_mask=attention_mask[i: i + model.max_chunks].to(\n",
    "                device, dtype=torch.long\n",
    "            ),\n",
    "            seq_ids=seq_ids[i: i + model.max_chunks].to(\n",
    "                device, dtype=torch.long\n",
    "            ),\n",
    "            category_ids=category_ids[i: i + model.max_chunks].to(\n",
    "                device, dtype=torch.long\n",
    "            ),\n",
    "            cutoffs=None,  # None,  # cutoffs, #None,\n",
    "            is_evaluation=True,\n",
    "            # note_end_chunk_ids=note_end_chunk_ids,\n",
    "        )\n",
    "        complete_sequence_output.append(sequence_output)\n",
    "        # complete_gate_lists.append(torch.mean(gate_lists, dim=2))\n",
    "        complete_gate_lists.append(gate_lists)\n",
    "        t += 1\n",
    "    # concatenate the sequence output\n",
    "    sequence_output = torch.cat(complete_sequence_output, dim=0)\n",
    "\n",
    "    # run through LWAN to get the scores\n",
    "    attn_output_weights, scores = return_attn_scores(model.label_attn, sequence_output, cutoffs=cutoffs)\n",
    "\n",
    "    labels_sample = []\n",
    "    for i in range(50):\n",
    "        if labels[i] == 1:\n",
    "            print(i)\n",
    "            labels_sample.append(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # visualize a heatmap of attn_output_weights[-1,l,:] for all l in 0,..,49 such that labels[l] == 1\n",
    "    # viz_label = 18\n",
    "\n",
    "\n",
    "    save_fig = True\n",
    "    if save_fig:\n",
    "\n",
    "        # visualize a heatmap of attn_output_weights[-1,l,:] for all l in 0,..,49 such that labels[l] == 1\n",
    "        # viz_label = 18\n",
    "        viz_label = 10\n",
    "\n",
    "        plot_gates = False\n",
    "        if plot_gates:\n",
    "            for cutoff in cutoffs.keys():\n",
    "                print(f\"Cutoff {cutoff}\")\n",
    "                cutoff_idx = cutoffs[cutoff]\n",
    "                print(f\"Label {viz_label}\")\n",
    "                plt.figure(figsize=set_size(438.17227, fraction=0.8))  # already used\n",
    "                attn_weights = attn_output_weights[cutoff_idx, viz_label, :].cpu().detach().numpy().reshape(1, -1)\n",
    "                most_important_chunk_idx = np.where(attn_weights == attn_weights.max())[1][0]\n",
    "\n",
    "                gates, types_gates = get_vector_at_index(complete_gate_lists, event_type_seq, index=most_important_chunk_idx)\n",
    "\n",
    "                plot_gate_importance(gates, types_gates, note_idx=most_important_chunk_idx, top_k=10,\n",
    "                                     save_path=f\"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/results/Multi_Modal_Multi_head_Residual_Attn_LN/figures/gate_weights_ch{most_important_chunk_idx}_{cutoff}.png\")\n",
    "\n",
    "                # make heatmap value range from the min to the max\n",
    "                sns.heatmap(\n",
    "                    attn_weights,\n",
    "                    cmap=\"Blues\",\n",
    "                    vmin=attn_weights.min(),\n",
    "                    vmax=attn_weights.max(),\n",
    "                )\n",
    "                # show lines between the notesº<\n",
    "                # which are in seq_ids\n",
    "                # for i in range(1, seq_ids.shape[0]):\n",
    "                #     if seq_ids[i-1] != seq_ids[i]:\n",
    "                #         plt.axvline(x=i, color=\"black\")\n",
    "                # show thin dotted lines at each cutoff\n",
    "                for cutoff in cutoffs.keys():\n",
    "                    cutoff_idx = cutoffs[cutoff]\n",
    "                    plt.axvline(x=cutoff_idx + 1, color=\"black\", linestyle=\":\")\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "        for cutoff in cutoffs.keys():\n",
    "            print(f\"Cutoff {cutoff}\")\n",
    "            cutoff_idx = cutoffs[cutoff]\n",
    "            print(f\"Label {viz_label}\")\n",
    "            plt.figure(figsize=set_size(438.17227, fraction=0.68))\n",
    "            attn_weights = attn_output_weights[cutoff_idx, viz_label, :].cpu().detach().numpy().reshape(1, -1)\n",
    "            # make heatmap value range from the min to the max\n",
    "            sns.heatmap(\n",
    "                attn_weights,\n",
    "                cmap=\"Blues\",\n",
    "                vmin=attn_weights.min(),\n",
    "                vmax=attn_weights.max(),\n",
    "            )\n",
    "            # show lines between the notesº<\n",
    "            # which are in seq_ids\n",
    "            # for i in range(1, seq_ids.shape[0]):\n",
    "            #     if seq_ids[i-1] != seq_ids[i]:\n",
    "            #         plt.axvline(x=i, color=\"black\")\n",
    "            # show thin dotted lines at each cutoff\n",
    "            cutoff_idxs = []\n",
    "            for cutoffx in cutoffs.keys():\n",
    "                if cutoffx != 'all':\n",
    "                    cutoff_idx = cutoffs[cutoffx]\n",
    "                    cutoff_idxs.append(cutoff_idx)\n",
    "                    plt.axvline(x=cutoff_idx + 1, color=\"black\", linestyle=\":\")\n",
    "            plt.xlabel(\"Chunk position\", fontsize=10)\n",
    "            # remove y axis ticks\n",
    "            plt.yticks([])\n",
    "            # rotate x ticks and place them every 5 ticks\n",
    "            plt.xticks(list(range(0, attn_weights.shape[1], 5)), list(range(0, attn_weights.shape[1], 5)), rotation=0,\n",
    "                       fontsize=8)\n",
    "            #\n",
    "            plt.savefig(\n",
    "                f\"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/Saved_models/LAHST/baseline/figures/new_attention_weights_{14}_{viz_label}_{cutoff}.png\",\n",
    "                bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "do_quantitative_eval = True\n",
    "\n",
    "print(\"doing quantitative qualitative evaluation...\")\n",
    "if do_quantitative_eval:\n",
    "    do_quantitative_analysis = True\n",
    "\n",
    "\n",
    "    if do_quantitative_analysis:\n",
    "        weights_per_class = {cutoff: {c: [] for c in range(15)} for cutoff in [\"2d\", \"5d\", \"13d\", \"noDS\", 'all']}\n",
    "        samples_per_class = {cutoff: {c: [] for c in range(15)} for cutoff in [\"2d\", \"5d\", \"13d\", \"noDS\", 'all']}\n",
    "\n",
    "        # Option 1: Soft normalized gating values\n",
    "        soft_contributions = {cutoff: {c: defaultdict(list) for c in range(15)} for cutoff in [\"2d\", \"5d\", \"13d\", \"noDS\", 'all']}\n",
    "\n",
    "        # Option 2: Top-k gating values (top 10)\n",
    "        topk_contributions = {cutoff: {c: defaultdict(list) for c in range(15)} for cutoff in [\"2d\", \"5d\", \"13d\", \"noDS\", 'all']}\n",
    "        top_k = 10\n",
    "\n",
    "        # Option 3: Thresholded gating values\n",
    "        threshold_contributions = {cutoff: {c: defaultdict(list) for c in range(15)} for cutoff in [\"2d\", \"5d\", \"13d\", \"noDS\", 'all']}\n",
    "        threshold = 0.1  # can be adjusted\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                for batch_idx, (hadm, event_type_sequence, sequences, sequences_tokenized, timestamps, labels) in tqdm(enumerate(\n",
    "                   validation_loader)):\n",
    "                    event_type_seq, seq, seq_tokenized, tstamp, input_ids, attention_mask, seq_ids, category_ids, cutoffs \\\n",
    "                        = select_sequence(event_type_sequence, sequences, sequences_tokenized, timestamps, tokenizer,\n",
    "                                          mode=\"eval\", num_text_chunks=16)  # to change to train\n",
    "\n",
    "                    if hadm == 113344:\n",
    "                        complete_sequence_output = []\n",
    "                        complete_gate_lists = []\n",
    "                        # run through data in chunks of max_chunks\n",
    "                        t = 0\n",
    "                        # run through data in chunks of max_chunks\n",
    "                        model.max_chunks = 16\n",
    "\n",
    "                        for i in range(0, input_ids.shape[0], model.max_chunks):\n",
    "                            # only get the document embeddings\n",
    "                            sequence_output, gate_lists = model(\n",
    "                                event_types=event_type_seq[t], seq_tokenized=seq_tokenized[t],\n",
    "                                input_ids=input_ids[i: i + model.max_chunks].to(\n",
    "                                    device, dtype=torch.long\n",
    "                                ),\n",
    "                                attention_mask=attention_mask[i: i + model.max_chunks].to(\n",
    "                                    device, dtype=torch.long\n",
    "                                ),\n",
    "                                seq_ids=seq_ids[i: i + model.max_chunks].to(\n",
    "                                    device, dtype=torch.long\n",
    "                                ),\n",
    "                                category_ids=category_ids[i: i + model.max_chunks].to(\n",
    "                                    device, dtype=torch.long\n",
    "                                ),\n",
    "                                cutoffs=None,  # None,  # cutoffs, #None,\n",
    "                                is_evaluation=True,\n",
    "                                # note_end_chunk_ids=note_end_chunk_ids,\n",
    "                            )\n",
    "                            complete_sequence_output.append(sequence_output)\n",
    "\n",
    "                            complete_gate_lists.append(torch.norm(gate_lists.cpu().detach(), dim=2).cpu().detach().numpy())\n",
    "                            t += 1\n",
    "                        # concatenate the sequence output\n",
    "                        sequence_output = torch.cat(complete_sequence_output, dim=0)\n",
    "                        del complete_sequence_output\n",
    "                        torch.cuda.empty_cache()\n",
    "                        attn_output_weights, scores = return_attn_scores(model.label_attn, sequence_output, cutoffs=cutoffs)\n",
    "                        labels_sample = []\n",
    "                        for i in range(50):\n",
    "                            if labels[i] == 1:\n",
    "                                labels_sample.append(i)\n",
    "                        for cutoff in cutoffs.keys():\n",
    "                            cutoff_idx = cutoffs[cutoff]\n",
    "                            for l in labels_sample:\n",
    "                                attn_weights = attn_output_weights[cutoff_idx, l, :].cpu().detach().numpy().reshape(1, -1)\n",
    "                                cutoff_idx = cutoffs[cutoff] if cutoff != 'all' else len(sequence_output) - 1\n",
    "                                for chunk in range(cutoff_idx + 1):\n",
    "                                    try:\n",
    "                                        c = category_ids[chunk].item()\n",
    "                                        weight = attn_output_weights[cutoff_idx, l, chunk].item()\n",
    "\n",
    "                                        gates, types_gates = get_vector_at_index(complete_gate_lists, event_type_seq,\n",
    "                                                                                 index=chunk)\n",
    "                                        # gates = gates.cpu().detach().numpy()\n",
    "                                        # Option 1: soft normalized contribution\n",
    "                                        total_gate = np.sum(gates) + 1e-8\n",
    "                                        for g_val, g_type in zip(gates, types_gates):\n",
    "                                            contribution = g_val / total_gate\n",
    "                                            soft_contributions[cutoff][c][g_type].append(contribution)\n",
    "\n",
    "                                        # Option 2: top-k\n",
    "                                        top_indices = np.argsort(gates)[-top_k:]\n",
    "                                        for idx in top_indices:\n",
    "                                            g_type = types_gates[idx]\n",
    "                                            g_val = gates[idx]\n",
    "                                            contribution = g_val /total_gate\n",
    "                                            topk_contributions[cutoff][c][g_type].append(contribution)\n",
    "\n",
    "                                        # Option 3: threshold\n",
    "                                        for g_val, g_type in zip(gates, types_gates):\n",
    "                                            contribution = g_val / total_gate\n",
    "                                            if contribution >= threshold:\n",
    "                                                threshold_contributions[cutoff][c][g_type].append(g_val)\n",
    "\n",
    "                                        weights_per_class[cutoff][c].append(weight)\n",
    "\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"[ERROR] chunk={chunk}, cutoff_idx={cutoff_idx}, label={l}\")\n",
    "                                        print(\n",
    "                                            f\"category_ids shape: {category_ids.shape}, attn_output_weights shape: {attn_output_weights.shape}\")\n",
    "                                        print(f\"cutoff={cutoff}, class={c if 'c' in locals() else 'undefined'}\")\n",
    "                                        print(f\"Exception: {type(e).__name__} - {e}\")\n",
    "                                        break  # optionally stop here to debug\n",
    "\n",
    "                        del sequence_output, gate_lists, input_ids, attention_mask, seq_ids, category_ids\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "\n",
    "                        #     cutoff_idx = cutoffs[cutoff]\n",
    "                        #     for l in labels_sample:\n",
    "                        #         attn_weights= attn_output_weights[cutoff_idx, l, :].cpu().detach().numpy().reshape(1, -1)\n",
    "                        #         class_weights = {i: 0 for i in range(14)}\n",
    "                        #         class_samples = {i: 0 for i in range(14)}\n",
    "                        #         for chunk in range(cutoff_idx+1):\n",
    "                        #             category = category_ids[chunk].item()\n",
    "                        #             class_weights[category] += attn_weights[0, chunk]\n",
    "                        #             class_samples[category] += 1\n",
    "                        #         for c in class_weights.keys():\n",
    "                        #             weights_per_class[cutoff][c].append(class_weights[c])\n",
    "                        #             samples_per_class[cutoff][c].append(class_samples[c])\n",
    "        json.dump(weights_per_class, open(\"weights_per_class.json\", 'w'))\n",
    "        json.dump(soft_contributions, open(\"soft_contributions_per_class.json\", 'w'))\n",
    "        json.dump(topk_contributions, open(\"topk_contributions_per_class.json\", 'w'))\n",
    "        json.dump(threshold_contributions, open(\"threshold_contributions_per_class.json\", 'w'))"
   ],
   "id": "1ef0d760a61b3660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, List, Optional\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_labels(splits):\n",
    "    split = pd.read_csv(splits)\n",
    "    unique_splits = split.SPLIT_50.unique()\n",
    "\n",
    "    # Collect all ICD codes into a list for counting\n",
    "    icd_counter = Counter()\n",
    "    for spl in tqdm(unique_splits):\n",
    "        temp = split[split[\"SPLIT_50\"] == spl]\n",
    "        print(f\"fetching the codes in {spl}\")\n",
    "        for hadm_id in temp[\"HADM_ID\"].unique():\n",
    "            icds_hadm = eval(temp[temp[\"HADM_ID\"] == hadm_id].absolute_code.iloc[0])\n",
    "            icd_counter.update(icds_hadm)\n",
    "\n",
    "    # Get the top 50 most common ICD codes\n",
    "    # Correct\n",
    "    top_50_icds = [icd for icd, _ in icd_counter.most_common(50)]\n",
    "\n",
    "    # Build mapping dicts only for the top 50\n",
    "    c2ind = {}\n",
    "    ind2c = {}\n",
    "    for i, icd in enumerate(top_50_icds):\n",
    "        c2ind[icd] = i\n",
    "        ind2c[i] = icd\n",
    "\n",
    "    print(\"done\")\n",
    "    return c2ind, ind2c\n",
    "\n",
    "# ---- Configuration ----\n",
    "MAIN_METRICS = [\"f1_macro\", \"f1_micro\", \"auc_macro\", \"auc_micro\", \"p_5\"]\n",
    "WINDOWS_ORDER = [\"2d\", \"5d\", \"13d\", \"noDS\", \"all\"]  # adjust order as you like\n",
    "\n",
    "def compare_models_by_label(models, metric_key=\"f1_by_class\", slice_key=\"all\", tie_mode=\"share\"):\n",
    "    \"\"\"\n",
    "    Compare models per label on a per-class metric (e.g., F1 or AUC).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    models: dict[str, dict]\n",
    "        model_name -> metrics dict (expects metrics[slice_key][metric_key] as list of floats)\n",
    "    metric_key: str\n",
    "        \"f1_by_class\" or \"auc_by_class\"\n",
    "    slice_key: str\n",
    "        e.g., \"all\", \"2d\", \"5d\", \"13d\", \"noDS\"\n",
    "    tie_mode: str\n",
    "        How to handle equal best scores for a label:\n",
    "        - \"share\": give the label to all tied models\n",
    "        - \"skip\": skip labels with ties (no winner)\n",
    "        - \"first\": break ties by first model encountered\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: dict with:\n",
    "        - \"winners_by_label\": list of tuples [(label_idx, [(model, score), ...best_ties]), ...]\n",
    "        - \"labels_won_by_model\": dict[model] -> sorted list of label indices it wins\n",
    "        - \"win_counts\": dict[model] -> number of labels won\n",
    "        - \"per_label_scores\": dict[label_idx] -> dict[model] -> score\n",
    "    \"\"\"\n",
    "    # Gather series and check max_len\n",
    "    series = {}\n",
    "    max_len = 0\n",
    "    for name, m in models.items():\n",
    "        arr = np.array(m[slice_key][metric_key], dtype=float)\n",
    "        series[name] = arr\n",
    "        max_len = max(max_len, len(arr))\n",
    "\n",
    "    winners_by_label = []\n",
    "    labels_won_by_model = defaultdict(list)\n",
    "    per_label_scores = {}\n",
    "\n",
    "    model_names = list(series.keys())\n",
    "\n",
    "    for lbl in range(max_len):\n",
    "        # collect available scores for this label\n",
    "        scores_here = {}\n",
    "        for name, arr in series.items():\n",
    "            if lbl < len(arr) and np.isfinite(arr[lbl]):\n",
    "                scores_here[name] = float(arr[lbl])\n",
    "\n",
    "        if not scores_here:\n",
    "            continue  # no data for this label across all models\n",
    "\n",
    "        per_label_scores[lbl+1] = scores_here  # 1-based label index for readability\n",
    "        # find best score\n",
    "        best_score = max(scores_here.values())\n",
    "        best_models = [n for n, s in scores_here.items() if np.isclose(s, best_score, rtol=1e-9, atol=1e-12)]\n",
    "\n",
    "        if tie_mode == \"skip\" and len(best_models) > 1:\n",
    "            continue\n",
    "        if tie_mode == \"first\" and len(best_models) > 1:\n",
    "            best_models = [model_names[0] if model_names[0] in best_models else best_models[0]]\n",
    "\n",
    "        winners_by_label.append((lbl+1, [(m, scores_here[m]) for m in best_models]))\n",
    "\n",
    "        # record wins\n",
    "        for m in best_models:\n",
    "            labels_won_by_model[m].append(lbl+1)\n",
    "\n",
    "    # counts\n",
    "    win_counts = {m: len(sorted(labels)) for m, labels in labels_won_by_model.items()}\n",
    "\n",
    "    # sort label lists for consistency\n",
    "    labels_won_by_model = {m: sorted(labels) for m, labels in labels_won_by_model.items()}\n",
    "\n",
    "    return {\n",
    "        \"winners_by_label\": winners_by_label,\n",
    "        \"labels_won_by_model\": labels_won_by_model,\n",
    "        \"win_counts\": win_counts,\n",
    "        \"per_label_scores\": per_label_scores,\n",
    "    }\n",
    "\n",
    "def load_metrics_folder(folder: str) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Load model metrics from JSON files in a folder.\n",
    "    Expected structure per file:\n",
    "      {\n",
    "        \"2d\": {\"f1_macro\": ..., \"f1_micro\": ..., ...},\n",
    "        \"5d\": {...}, \"13d\": {...}, \"noDS\": {...}, \"all\": {...}\n",
    "      }\n",
    "    Returns: {model_name: metrics_dict}\n",
    "    model_name = file stem (filename without extension)\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for path in glob.glob(os.path.join(folder, \"*.json\")):\n",
    "        name = os.path.splitext(os.path.basename(path))[0]\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        result[name] = data\n",
    "    return result\n",
    "\n",
    "def plot_labels_by_model(models, slice_key=\"all\"):\n",
    "    \"\"\"\n",
    "    models: dict[str, dict] mapping model_name -> metrics dict\n",
    "            expects metrics[slice_key] with keys 'f1_by_class' and 'auc_by_class'\n",
    "            optionally 'f1_macro' and 'auc_macro' for horizontal reference lines\n",
    "    slice_key: one of 'all', '2d', '5d', '13d', 'noDS', etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Collect and align lengths ---\n",
    "    f1_series = {}\n",
    "    auc_series = {}\n",
    "    max_len = 0\n",
    "    for name, metrics in models.items():\n",
    "        f1 = metrics[slice_key][\"f1_by_class\"]\n",
    "        auc = metrics[slice_key][\"auc_by_class\"]\n",
    "        f1_series[name] = np.array(f1, dtype=float)\n",
    "        auc_series[name] = np.array(auc, dtype=float)\n",
    "        max_len = max(max_len, len(f1))\n",
    "\n",
    "    labels = np.arange(1, max_len + 1)\n",
    "\n",
    "    # Nice distinct markers to help separate models\n",
    "    marker_cycler = cycle([\"o\", \"s\", \"^\", \"D\", \"P\", \"X\", \"v\", \"<\", \">\", \"*\"])\n",
    "\n",
    "    # --- F1 plot ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, f1 in f1_series.items():\n",
    "        x = labels[:len(f1)]\n",
    "        marker = next(marker_cycler)\n",
    "        plt.scatter(x, f1, alpha=0.7, label=name, marker=marker)\n",
    "\n",
    "    # Horizontal macro average lines if available\n",
    "    for name, metrics in models.items():\n",
    "        macro = metrics[slice_key].get(\"f1_macro\", None)\n",
    "        if macro is not None:\n",
    "            plt.axhline(macro, linestyle=\"--\", linewidth=0.8, alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Class index\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.title(f\"Per-class F1 scores ({slice_key})\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(title=\"Model\", ncols=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # reset marker cycle for AUC plot\n",
    "    marker_cycler = cycle([\"o\", \"s\", \"^\", \"D\", \"P\", \"X\", \"v\", \"<\", \">\", \"*\"])\n",
    "\n",
    "    # --- AUC plot ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, auc in auc_series.items():\n",
    "        x = labels[:len(auc)]\n",
    "        marker = next(marker_cycler)\n",
    "        plt.scatter(x, auc, alpha=0.7, label=name, marker=marker)\n",
    "\n",
    "    for name, metrics in models.items():\n",
    "        macro = metrics[slice_key].get(\"auc_macro\", None)\n",
    "        if macro is not None:\n",
    "            plt.axhline(macro, linestyle=\"--\", linewidth=0.8, alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Class index\")\n",
    "    plt.ylabel(\"AUC score\")\n",
    "    plt.title(f\"Per-class AUC scores ({slice_key})\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(title=\"Model\", ncols=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_grouped_bars(\n",
    "    models_to_metrics: Dict[str, Dict[str, Dict[str, Any]]],\n",
    "    metrics: Optional[List[str]] = None,\n",
    "    windows_order: Optional[List[str]] = None,\n",
    "    save_dir: Optional[str] = None,\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create one bar chart per metric. X-axis = model names; grouped by time windows.\n",
    "    models_to_metrics: {model_name: {window: {metric: value, ...}, ...}, ...}\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = MAIN_METRICS\n",
    "    if windows_order is None:\n",
    "        # determine a stable window order present in the data\n",
    "        present = set()\n",
    "        for m in models_to_metrics.values():\n",
    "            present |= set(m.keys())\n",
    "        windows_order = [w for w in WINDOWS_ORDER if w in present] + sorted(present - set(WINDOWS_ORDER))\n",
    "\n",
    "    model_names = list(models_to_metrics.keys())\n",
    "    n_models = len(model_names)\n",
    "    n_windows = len(windows_order)\n",
    "    x = np.arange(n_models)\n",
    "    bar_width = 0.8 / max(1, n_windows)\n",
    "\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(max(8, n_models * 1.2), 5))\n",
    "\n",
    "        # Plot each window as a bar group\n",
    "        for i, window in enumerate(windows_order):\n",
    "            vals = []\n",
    "            for model_name in model_names:\n",
    "                # Safely fetch the metric value (NaN if missing)\n",
    "                val = np.nan\n",
    "                if window in models_to_metrics[model_name]:\n",
    "                    val = models_to_metrics[model_name][window].get(metric, np.nan)\n",
    "                vals.append(val)\n",
    "\n",
    "            # Handle NaNs by replacing with 0 for plotting, and mark in labels if you want.\n",
    "            vals_np = np.array(vals, dtype=float)\n",
    "            plt.bar(x + (i - n_windows/2) * bar_width + bar_width/2, np.nan_to_num(vals_np, nan=0.0),\n",
    "                    width=bar_width, label=window)\n",
    "\n",
    "        plt.title(metric)\n",
    "        plt.xticks(x, model_names, rotation=20, ha=\"right\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend(title=\"Window\", ncols=min(n_windows, 4))\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            out_path = os.path.join(save_dir, f\"{metric}.png\")\n",
    "            plt.savefig(out_path, dpi=150)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def plot_grouped_by_time(\n",
    "        models_to_metrics,\n",
    "        metrics=(\"f1_macro\", \"f1_micro\", \"auc_macro\", \"auc_micro\", \"p_5\", \"LRAP\"),\n",
    "        windows_order=(\"2d\", \"5d\", \"13d\", \"noDS\", \"all\"),\n",
    "        decimals=2,\n",
    "        save_dir=None,\n",
    "        show=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    One figure per metric.\n",
    "    X-axis: time windows. Bars within each group: models (different colors).\n",
    "    Text labels: value printed on each bar.\n",
    "    \"\"\"\n",
    "    model_names = list(models_to_metrics.keys())\n",
    "    n_models = len(model_names)\n",
    "    windows_order = [w for w in windows_order if any(w in m for m in models_to_metrics.values())]\n",
    "    n_windows = len(windows_order)\n",
    "\n",
    "    x = np.arange(n_windows)\n",
    "    bar_width = 0.8 / max(1, n_models)\n",
    "\n",
    "    def _val(mname, window, metric):\n",
    "        try:\n",
    "            return models_to_metrics[mname][window].get(metric, np.nan)\n",
    "        except KeyError:\n",
    "            return np.nan\n",
    "\n",
    "    def _annotate(ax, rects, values, decimals=3):\n",
    "        for rect, v in zip(rects, values):\n",
    "            if np.isnan(v):\n",
    "                continue\n",
    "            height = rect.get_height()\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2.0,\n",
    "                height + 0.01,  # little padding above bar\n",
    "                f\"{v:.{decimals}f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=9, rotation=0\n",
    "            )\n",
    "\n",
    "    for metric in metrics:\n",
    "        fig, ax = plt.subplots(figsize=(max(8, n_windows * 1.6), 5))\n",
    "\n",
    "        max_val = 0.0\n",
    "        for i, mname in enumerate(model_names):\n",
    "            vals = [_val(mname, w, metric) for w in windows_order]\n",
    "            vals_np = np.array(vals, dtype=float)\n",
    "            max_val = max(max_val, np.nanmax(np.nan_to_num(vals_np, nan=0.0)))\n",
    "\n",
    "            rects = ax.bar(\n",
    "                x + (i - n_models / 2) * bar_width + bar_width / 2,\n",
    "                np.nan_to_num(vals_np, nan=0.0),\n",
    "                width=bar_width,\n",
    "                label=mname\n",
    "            )\n",
    "            _annotate(ax, rects, vals_np, decimals=decimals)\n",
    "\n",
    "        # Titles and labels\n",
    "        ax.set_title(f\"Performance on {metric}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xticks(x, windows_order)\n",
    "        ax.set_xlabel(\"Time Window\")\n",
    "        ax.set_ylabel(metric)\n",
    "\n",
    "        # Legend outside\n",
    "        ax.legend(title=\"Model\", ncols=1, bbox_to_anchor=(1.02, 1), loc=\"upper left\", borderaxespad=0)\n",
    "\n",
    "        # Add grid and headroom\n",
    "        ax.set_ylim(0, max(1.0 if max_val <= 1.0 else max_val, max_val * 1.12))\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            fig.savefig(os.path.join(save_dir, f\"grouped_by_time__{metric}.png\"),\n",
    "                        dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "# ---------- Example usage ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Option A) Load from files in a folder (each file = one model's metrics)\n",
    "    # models = load_metrics_folder(\"./metrics_runs\")\n",
    "\n",
    "    # Option B) Build from in-memory dicts (using your example for one model)\n",
    "    # Add as many models as you want in this dict (modelA, modelB, ...)\n",
    "\n",
    "    with open(\n",
    "            \"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/results/LAHST/baseline/TEST_MMULA_evaluate.json\") as f:\n",
    "        modelA = json.load(f)\n",
    "\n",
    "    with open(\n",
    "            \"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/results/LAHST_Hierarchy/TEST_MMULA_evaluate.json\") as f:\n",
    "        modelB = json.load(f)\n",
    "\n",
    "    with open(\n",
    "            \"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/results/Multi_Modal_GATING/Multi_Modal_LAHST_Active_LN/TEST_Multi_modal.json\") as f:\n",
    "        modelC = json.load(f)\n",
    "\n",
    "    with open(\n",
    "            \"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/results/Multi_Modal_Multi_head_Residual_Attn_LN/TEST_Multi_modal.json\") as f:\n",
    "        modelD = json.load(f)\n",
    "\n",
    "    with open(\n",
    "            \"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/results/HLAST_MH_Residual_Attn_LN/TEST_MMULA_evaluate.json\") as f:\n",
    "        modelE = json.load(f)\n",
    "\n",
    "\n",
    "    models = {\n",
    "        \"LAHST\": modelA,\n",
    "        # \"LAHST_H\": modelB,\n",
    "        \"Multi_MG\": modelC,\n",
    "        \"Multi_MMH\": modelD,\n",
    "        #\"HLAST++\": modelE,\n",
    "\n",
    "        # \"ModelC\": load from file or construct similarly...\n",
    "    }\n",
    "\n",
    "    c2ind, ind2c = get_labels(\"/Users/p-a/PycharmProjects/ICD_Coding/Multi_Modal_Continuous/Data/splits/caml_splits.csv\")\n",
    "\n",
    "\n",
    "    plot_labels_by_model(models, slice_key=\"2d\")\n",
    "    plot_labels_by_model(models, slice_key=\"5d\")\n",
    "    plot_labels_by_model(models, slice_key=\"13d\")\n",
    "    plot_labels_by_model(models, slice_key=\"noDS\")\n",
    "    plot_labels_by_model(models, slice_key=\"all\")\n",
    "\n",
    "    # --------- Example usage ----------\n",
    "    result = compare_models_by_label(models, metric_key=\"f1_by_class\", slice_key=\"all\", tie_mode=\"share\")\n",
    "    print(\"Win counts all:\", result[\"win_counts\"])\n",
    "    print(\"Labels won by model all:\", result[\"labels_won_by_model\"])\n",
    "    # # Show first 10 labels’ winners:\n",
    "    print(result[\"winners_by_label\"][:10])\n",
    "\n",
    "    # To compare by AUC:\n",
    "    result_auc = compare_models_by_label(models, metric_key=\"auc_by_class\", slice_key=\"all\", tie_mode=\"share\")\n",
    "\n",
    "    # ------ 2days ----------\n",
    "    result = compare_models_by_label(models, metric_key=\"f1_by_class\", slice_key=\"2d\", tie_mode=\"share\")\n",
    "    print(\"Win counts 2d:\", result[\"win_counts\"])\n",
    "    print(\"Labels won by model 2d:\", result[\"labels_won_by_model\"])\n",
    "    # # Show first 10 labels’ winners:\n",
    "    print(result[\"winners_by_label\"][:10])\n",
    "\n",
    "    # To compare by AUC:\n",
    "    result_auc = compare_models_by_label(models, metric_key=\"auc_by_class\", slice_key=\"2d\", tie_mode=\"share\")\n",
    "\n",
    "    # ------ 5days ----------\n",
    "    result = compare_models_by_label(models, metric_key=\"f1_by_class\", slice_key=\"5d\", tie_mode=\"share\")\n",
    "    print(\"Win counts 5d:\", result[\"win_counts\"])\n",
    "    print(\"Labels won by model 5d:\", result[\"labels_won_by_model\"])\n",
    "    # # Show first 10 labels’ winners:\n",
    "    print(result[\"winners_by_label\"][:10])\n",
    "\n",
    "    # To compare by AUC:\n",
    "    result_auc = compare_models_by_label(models, metric_key=\"auc_by_class\", slice_key=\"5d\", tie_mode=\"share\")\n",
    "\n",
    "    # ------ 13days ----------\n",
    "    result = compare_models_by_label(models, metric_key=\"f1_by_class\", slice_key=\"13d\", tie_mode=\"share\")\n",
    "    print(\"Win counts 13d:\", result[\"win_counts\"])\n",
    "    print(\"Labels won by model 13d:\", result[\"labels_won_by_model\"])\n",
    "    # # Show first 10 labels’ winners:\n",
    "    print(result[\"winners_by_label\"][:10])\n",
    "\n",
    "    # To compare by AUC:\n",
    "    result_auc = compare_models_by_label(models, metric_key=\"auc_by_class\", slice_key=\"13d\", tie_mode=\"share\")\n",
    "\n",
    "    # ------ noDS ----------\n",
    "    result = compare_models_by_label(models, metric_key=\"f1_by_class\", slice_key=\"noDS\", tie_mode=\"share\")\n",
    "    print(\"Win counts noDS:\", result[\"win_counts\"])\n",
    "    print(\"Labels won by model noDS:\", result[\"labels_won_by_model\"])\n",
    "    # # Show first 10 labels’ winners:\n",
    "    print(result[\"winners_by_label\"][:10])\n",
    "\n",
    "    # To compare by AUC:\n",
    "    result_auc = compare_models_by_label(models, metric_key=\"auc_by_class\", slice_key=\"noDS\", tie_mode=\"share\")\n",
    "\n",
    "    plot_grouped_bars(\n",
    "        models_to_metrics=models,\n",
    "        metrics=[\"f1_macro\", \"f1_micro\", \"auc_macro\", \"auc_micro\", \"p_5\", \"LRAP\"],\n",
    "        windows_order=[\"2d\", \"5d\", \"13d\", \"noDS\", \"all\"],  # choose any subset/order\n",
    "        save_dir=None,   # e.g., \"./plots\"\n",
    "        show=True\n",
    "    )\n",
    "    plot_grouped_by_time(\n",
    "        models_to_metrics=models,\n",
    "        metrics=(\"f1_macro\", \"f1_micro\", \"auc_macro\", \"auc_micro\", \"p_5\", \"LRAP\"),\n",
    "        windows_order=(\"2d\", \"5d\", \"13d\", \"noDS\", \"all\"),\n",
    "        decimals=2,\n",
    "        save_dir=None,  # or a folder path like \"./plots_time_grouped\"\n",
    "        show=True\n",
    "    )"
   ],
   "id": "52a29f87e058f57b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
